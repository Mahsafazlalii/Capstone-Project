import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv
import os
import logging

class APICataloger:
    def __init__(self):
        self.catalog = {}

    def add_api(self, title, link, title2, link2, json_link, combined):
        self.catalog[title] = {"Link": link, "Title for Link2": title2, "Link2": link2, "JSON Link": json_link, "Combined": combined}

    def get_catalog(self):
        return self.catalog

visited_links = set()
excluded_titles = ["SOAP", "Legend", "All Layers and Tables", "Dynamic Legend", "Dynamic Layer", "Export Map",
                   "Dynamic All Layers", "Metadata", "info", "Iteminfo", "Thumbnail", "Generate Renderer",
                   "Query", "WMS", "ArcGIS Earth", "QueryLegends", "QueryDomains", "Info", "Identify",
                   "Query Attachments", "Generate KML"]

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def scrape_links(url, csv_writer):
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.RequestException as e:
        logging.error(f"Request failed for {url}: {e}")
        return

    soup = BeautifulSoup(response.content, 'html.parser')
    links = soup.find_all('a')

    for link in links:
        href = link.get('href')
        title = link.text.strip()
        if href and title and "/services/" in href and "?f=" not in href and title not in excluded_titles:
            full_link = urljoin(url, href)
            if full_link not in visited_links:
                visited_links.add(full_link)
                if full_link.endswith('MapServer'):
                    process_map_server(full_link, title, csv_writer)
                else:
                    scrape_links(full_link, csv_writer)
            else:
                logging.info(f"Skipping already visited link: {full_link}")

def process_map_server(full_link, title, csv_writer):
    json_link = f"{full_link}/query?where=1%3D1&outFields=*&returnGeometry=false&f=json"
    sublayers = get_sublayers(full_link)
    for sublayer in sublayers:
        sublayer_link = f"{full_link}/{sublayer['id']}"
        combined = f"{title} {sublayer['name']}"
        csv_writer.writerow([title, full_link, sublayer['name'], sublayer_link, json_link, combined])

def get_sublayers(url):
    try:
        response = requests.get(url + '?f=json')
        response.raise_for_status()
        data = response.json()
        if 'layers' in data:
            return [{'id': layer['id'], 'name': layer['name']} for layer in data['layers']]
    except requests.RequestException as e:
        logging.error(f"Failed to get sublayers from {url}: {e}")
    return []

def main():
    starting_url = 'https://www.apps.geomatics.gov.nt.ca/arcgis/rest/services'

    csv_path = r'C:\\NLSA_Toolkit\\Data_Links\\input.csv'

    # Check if the directory exists, if not create it
    directory = os.path.dirname(csv_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

    # Attempt to write to the CSV file
    try:
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerow(['Title for Link', 'Link', 'Title for Link2', 'Link2', 'JSON Link', 'Combined'])
            scrape_links(starting_url, csv_writer)
        logging.info("Process completed, please check the destination folder to view your CSV")
    except Exception as e:
        logging.error(f"An error occurred: {e}")

if __name__ == "__main__":
    main()

